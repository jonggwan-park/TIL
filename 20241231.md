# 머신러닝 연습해보기

## 결측치 제거 (dropna)
```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error,r2_score,mean_absolute_error
df = pd.read_csv('housingdata.csv')

columns = ['CRIM','ZN','INDUS','CHAS','NOX','RM','AGE','DIS','RAD','TAX','PTRATIO','B','LSTAT','MEDV']
df = df.dropna()
# 데이터에서 결측치가 있는 행 삭제
```
## 결측치 제거 (최빈값)
```python
imputer = SimpleImputer(strategy='most_frequent')
df=pd.DataFrame(imputer.fit_transform(df), columns=df.columns)
#print(df)
```

## 이상치 제거 IsolationForest
이상치의 비율을 정해 놓고 해당 비율만큼 제거
```python
iso = IsolationForest(contamination=0.1, random_state=42) 
# contamination='auto'로 설정하면, 이상치 비율을 자동으로 추정합니다.
labels = iso.fit_predict(df)
outliers = df[labels==-1]
df_cleaned = df[labels!=-1]
print(df_cleaned)
```

## 이상치 제거 IQR
```python
Q1 = df.quantile(0.25)
Q3 = df.quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 -1.5*IQR
upper_bound = Q3 +1.5*IQR
outliers = df[(df<lower_bound) | (df>upper_bound)]
df_cleaned = df[(df>=lower_bound) & (df<=upper_bound)]
#IQR로 이상치 제거
#print(df_cleaned)
df_cleaned = df_cleaned.dropna()

X=df_cleaned.drop('MEDV',axis=1) # feature
Y=df_cleaned['MEDV'] # lable
X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.2,random_state=42)
# 테스트와 트레인으로 나눠줌 2대8비율
#print("train_data_size",X_train.shape,Y_train.shape)
#print("test_data_",X_test.shape,Y_test.shape)
```
## 이상치 제거 z-score
```python
z_score = df.apply(lambda x : (x-x.mean())/x.std())
#print(z_score.head(10))
outliers = z_score.abs() >3
df['is outlier'] = outliers.any(axis=1)
#df_cleaned = df[~outliers.any(axis=1)]
print(df)
```

## train_test_split 
```python
X=df_cleaned.drop('MEDV',axis=1) # feature
y=df_cleaned['MEDV'] # lable
X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42)
# 테스트와 트레인으로 나눠줌 2대8비율
#print("train_data_size",X_train.shape,Y_train.shape)
#print("test_data_",X_test.shape,Y_test.shape)
```

## 선형회기
```python
model = LinearRegression()
model.fit(X_train,Y_train)
Y_pred = model.predict(X_test)
mse = mean_squared_error(Y_test,Y_pred)
mae = mean_absolute_error(Y_test,Y_pred)
r2 = r2_score(Y_test,Y_pred)

print(f"평균 제곱 오차(mse): {mse:.2f}")
print(f"평균 절대 오차 (mae): {mae:.2f}")
print(f"결정계수(r**2): {r2:.2f}")

# 예측 값과 실제 값 비교
plt.figure(figsize=(10, 6))
plt.scatter(Y_test, Y_pred, alpha=0.7, color='blue', label='Predicted vs Actual')
plt.plot([Y_test.min(), Y_test.max()], [Y_test.min(), Y_test.max()], '--r', linewidth=2, label='Perfect Fit Line')
plt.title("Actual vs Predicted Values")
plt.xlabel("Actual MEDV")
plt.ylabel("Predicted MEDV")
plt.legend()
plt.show()
```
## 의사결정나무
```python
from sklearn.tree import DecisionTreeRegressor, plot_tree
model = DecisionTreeRegressor(max_depth=5,random_state=42)
model.fit(X_train,Y_train)
```