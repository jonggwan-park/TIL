## Transformer
Transformer는 자연어 처리(NLP)에서 뛰어난 성능을 보이는 모델입니다. Self-Attention 메커니즘을 활용해 텍스트의 문맥을 파악하고, 병렬 처리에 강한 구조를 가지고 있습니다. BERT, GPT, T5 같은 유명한 모델들이 모두 Transformer 기반입니다.

### PyTorch 설치 및 기본 설정
```shell
pip install torch torchvision torchaudio
```
```python
import torch
import torch.nn as nn
from torch.nn import Transformer

model = Transformer(d_model=512, nhead=8, num_encoder_layers=6, num_decoder_layers=6)
```
d_model 은 차원의 갯수
nhead PyTorch에서 **nhead**는 주로 Transformer 모델의 Multi-Head Attention 메커니즘에서 사용됩니다. 이는 torch.nn.Transformer 또는 torch.nn.MultiheadAttention 모듈에서 중요한 하이퍼파라미터로 설정됩니다.

Multi-Head Attention의 기본 개념
Attention Mechanism: 입력 시퀀스의 각 요소가 다른 요소와 얼마나 관련이 있는지를 계산합니다.
Multi-Head Attention:
Attention 메커니즘을 병렬로 여러 번 수행하여 모델이 입력 데이터의 다양한 부분에 대해 더 잘 학습할 수 있도록 합니다.
각 Attention "Head"는 서로 다른 부분의 정보를 학습하고, 이를 결합하여 풍부한 표현을 만듭니다.

nhead의 의미
**nhead**는 Multi-Head Attention에서 사용하는 Attention Head의 수를 지정합니다.
예를 들어, nhead=8로 설정하면 8개의 병렬 Attention 메커니즘이 실행됩니다.
주요 역할:
병렬성:
여러 개의 Attention Head가 서로 다른 부분의 패턴을 학습합니다.
더 많은 Head는 모델의 표현력을 증가시킬 수 있습니다.
차원 분할:
각 Head는 입력 임베딩의 차원을 균등하게 분할하여 연산합니다.
예를 들어, 입력 임베딩 차원이 512이고 nhead=8인 경우, 각 Head는 차원 512/8 = 64를 처리합니다.

nhead 선택 시 주의사항
임베딩 차원과의 관계:

입력 임베딩 차원(d_model)은 nhead로 나누어떨어져야 합니다.
예: d_model=512, nhead=8 → Head별 차원: 
512/8=64.
나누어떨어지지 않으면 모델 생성 시 에러가 발생합니다.
nhead의 크기:

너무 작은 nhead는 Attention의 병렬성이 부족해 모델 성능이 저하될 수 있습니다.
너무 큰 nhead는 연산량이 증가하며, 각 Head가 처리하는 정보가 과도하게 분할될 수 있습니다.
모델 크기와 데이터에 맞춤:

일반적으로 nhead는 8, 16 등의 값을 사용하며, 데이터와 모델 크기에 따라 조정합니다.
## Optimizer


**Optimizer(최적화 알고리즘)**는 딥러닝 모델의 **가중치(weight)**를 학습 데이터에 맞게 업데이트하는 역할을 합니다. 모델의 손실 함수(loss function)를 최소화하기 위해 가중치 값을 조정하며, 이를 통해 모델이 데이터를 더 잘 학습하도록 합니다.

Optimizer의 역할
손실 함수 계산:

손실 함수는 모델의 예측 값과 실제 값 간의 차이를 나타냅니다.
Optimizer는 이 손실 값을 기반으로 가중치를 조정합니다.
기울기(Gradient) 기반 업데이트:

역전파(Backpropagation)를 통해 계산된 손실 함수의 기울기를 사용하여 가중치를 조정합니다.
기울기 정보는 모델의 가중치가 어떻게 변경되어야 손실이 줄어드는지 방향을 알려줍니다.
학습율(Learning Rate) 조정:

Optimizer는 학습율을 사용하여 가중치를 얼마나 크게 업데이트할지 결정합니다.

### SGD (Stochastic Gradient Descent)
가장 기본적인 최적화 알고리즘.
모든 데이터를 사용하지 않고, 데이터의 **일부 샘플(배치)**를 사용하여 가중치를 업데이트.

```python
import torch
optimizer = torch.optim.SGD(model.parameters(), lr=0.01)
```

### Adam(Adaptive Moment Estiation)
Adam (Adaptive Moment Estimation)
SGD의 개선 버전으로, 모멘텀과 적응 학습율을 사용.
학습율을 동적으로 조정하여 빠르고 안정적인 학습을 제공합니다.
공식:
1차 모멘텀(기울기의 이동 평균): 
m t
​

2차 모멘텀(기울기 제곱의 이동 평균): 
v t
```python
​import torch
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
```

### RMSProp
기울기 제곱의 이동 평균을 사용하여 학습율을 조정.
주로 **순환 신경망(RNN)**에서 성능이 좋음.
```python
import torch
optimizer = torch.optim.RMSprop(model.parameters(), lr=0.01)
```

### Adagrad
각 파라미터마다 적응 학습율을 사용.
드물게 나타나는 피처에 더 큰 업데이트를 적용.
```python
import torch
optimizer = torch.optim.Adagrad(model.parameters(), lr=0.01)
```

Optimizer 선택 기준
데이터 특성:
    대규모 데이터: Adam, RMSProp.
    소규모 데이터: SGD.
모델 특성:
    순환 신경망(RNN): RMSProp, Adam.
    심층 신경망: Adam.
학습 속도:
    Adam은 일반적으로 SGD보다 빠르게 수렴.


```python
optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)
criterion = nn.CrossEntropyLoss()

for epoch in range(num_epochs):
    optimizer.zero_grad()
    output = model(src, tgt)
    loss = criterion(output, tgt_labels)
    loss.backward()
    optimizer.step()

```
PyTorch 허브(Pytorch Hub)를 이용하면, 사전 학습된 다양한 모델들을 손쉽게 활용할 수 있습니다. 예를 들어, torch.hub.load()를 사용해 GPT-2 같은 사전 학습된 모델을 불러올 수 있습니다.
```python
!pip install sentencepiece sacremoses
```
사전 학습된 모델 활용 예시
```python
model = torch.hub.load('huggingface/pytorch-transformers', 'modelForCausalLM', 'gpt2')
tokenizer = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', 'gpt2')
```

불러온 모델 실행
```python
input_text = "Once upon a time"
```

토큰화 진행
```python
input_ids = tokenizer.encode(input_text, return_tensors='pt')
```

return_tensors
설명: 반환 데이터를 텐서(tensor) 형식으로 변환합니다.
기본적으로, tokenizer.encode는 Python 리스트로 토큰 ID를 반환합니다. 하지만 모델에 입력하려면 텐서 형식이 필요하므로, return_tensors를 사용해 변환합니다.
가능한 옵션:
return_tensors='pt':
데이터를 PyTorch 텐서(torch.Tensor)로 반환합니다.
PyTorch를 사용하는 모델에 적합합니다.
return_tensors='tf':
데이터를 TensorFlow 텐서(tf.Tensor)로 반환합니다.
TensorFlow를 사용하는 모델에 적합합니다.
return_tensors=None (기본값):
데이터를 리스트 형태로 반환합니다.

List 형태: [101, 7592, 1010, 2129, 2024, 2017, 102]

PyTorch Tensor 형태: tensor([[ 101, 7592, 1010, 2129, 2024, 2017,  102]])



아웃풋 생성
```python
output = model.generate(input_ids, max_length = 50, num_return_sequences = 1)
```

아웃풋 토큰화 진행
```python
tokenizer.decode(output[0], skip_special_tokens = True)
```

### 대형 모델의 학습이 힘들어요
1️⃣ **데이터 및 컴퓨팅 자원의 한계**


**`Transformer`** 모델은 특히나 방대한 데이터를 필요로 하고, 학습에 많은 시간이 걸립니다. 일반적으로 수십 기가바이트(GPU 메모리) 이상이 필요하며, 몇 주간의 학습 시간이 필요할 수 있어요.




2️⃣ **모델 크기와 메모리 사용량**

모델이 커질수록 메모리 사용량도 기하급수적으로 늘어납니다. 개인이 보유한 일반적인 컴퓨터나 단일 GPU로는 이 대형 모델들을 학습시키기가 매우 어려웁니다.

### 복잡한 모델은 직접 만들기 힘들어요

1️⃣ **구현의 어려움**

**`Transformer`** 같은 모델은 구조가 복잡해서 처음부터 직접 구현하려면 많은 지식과 경험이 필요합니다. **`Self-Attention`**, **`Position-wise Feed-Forward Networks`**, **`Multi-Head Attention`** 등 여러 개념을 잘 이해해야 합니다.




2️⃣ **하이퍼파라미터 튜닝**

학습률, 모델 크기, 레이어 수 등 다양한 하이퍼파라미터를 적절히 조절해야 하는데, 이 과정에서 시행착오가 많을 수 있습니다. 특히, 최적의 파라미터를 찾기 위해서는 많은 실험과 시간이 필요합니다.

###  사전 학습된 모델 활용의 한계

- **맞춤화의 어려움**: 사전 학습된 모델은 특정 데이터나 작업에 대해 학습된 상태이기 때문에, 다른 작업에 맞추려면 추가적인 미세 조정(Fine-Tuning)이 필요합니다.
- **비용 문제**: 미세 조정이나 추가 학습을 하려면 대형 클라우드 서비스나 고성능 장비가 필요할 수 있으며, 이는 상당한 비용을 요구할 수 있습니다.
